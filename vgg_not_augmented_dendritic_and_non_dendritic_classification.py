# -*- coding: utf-8 -*-
"""VGG - Not augmented Dendritic and non_Dendritic Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T9BwuPXegzHvhaLgqvY52jidau9q5u3g
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import skimage.io
import os
import keras
import os
import glob
# %matplotlib inline
import shutil
from shutil import copyfile
import random
import time
import cv2

# tensorflow Libraries
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
# from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# sklearn Libraries
import sklearn
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# Metrics Libraries
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score
from sklearn.metrics import precision_score, f1_score, ConfusionMatrixDisplay
from sklearn.metrics import classification_report

"""# Import dataset from google drive"""

# Download dataset
!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/nyb6mycvfd-1.zip

#unzip the dataset
!unzip -q /content/nyb6mycvfd-1.zip

"""# Image preprocessing"""



try:
  os.mkdir('main_dir')
except:
  print("Folder already exists")

# Move Dendritic folder
try:
  shutil.move('/content/Dendritic', '/content/main_dir')
except:
  print("Folder already exists")

# Move Non-Dendritic folder
try:
  shutil.move('/content/Non-Dendritic', '/content/main_dir')
except:
  print("Folder already exists")

! pip install split-folders

import splitfolders #Import the splitfolders library
import os
print(os.listdir('/content/main_dir'))
splitfolders.ratio('/content/main_dir', output="output", seed=1337, ratio=(.8, .2), group_prefix=None) #Call the splitfolders function

# set path to dendritic micrograph images
dendritic_dir = '/content/main_dir/Dendritic'
# set path to non dendritic micrograph images
non_dendritic_dir = '/content/main_dir/Non-Dendritic'

#Print the total number of images in each directory
print("The total number of dendritic images are", len(os.listdir(dendritic_dir)))
print("The total number of non dendritic images are", len(os.listdir(non_dendritic_dir)))

# Data Visualization
import matplotlib.image as mpimg

# Setting the no of rows and columns
ROWS = 4
COLS = 4

# Setting the figure size
fig = plt.gcf()
fig.set_size_inches(12, 12)

# get the directory to each image file in the trainset
non_dendritic_pic = [os.path.join(non_dendritic_dir, i) for i in os.listdir(non_dendritic_dir)[:8]]
dendritic_pic = [os.path.join(dendritic_dir, i) for i in os.listdir(dendritic_dir)[:8]]

# merge dedritic and non_dendritic images
merged_list = dendritic_pic + non_dendritic_pic
print("The total number of images in the merged list are", len(merged_list))

# Plotting the images in the merged list
for i, img_path in enumerate(merged_list):
    # getting the filename from the directory
    data = img_path.split('/', 7)[3]
    # creating a subplot of images with the no. of rows and colums with index no
    sp = plt.subplot(ROWS, COLS, i+1)
    # turn off axis
    sp.axis('Off')
    # reading the image data to an array
    img = mpimg.imread(img_path)
    # setting title of plot as the filename
    sp.set_title(data, fontsize=10)
    # displaying data as image
    plt.imshow(img, cmap='gray')

plt.show()  # display the plot

# Plot class distribution
plt.figure(figsize=(8,2))
x = np.arange(2)
y = [len(os.listdir(dendritic_dir)), len(os.listdir(non_dendritic_dir))]
plt.barh(x, y)
plt.yticks(x, ["Dendritic", "Non-Dendritic"], fontsize=10)
plt.text(y[0]+5, x[0], y[0], fontsize=8)
plt.text(y[1]+5, x[1], y[1], fontsize=8)
plt.title("Distribution of Classes in the Dataset", fontsize=14);

"""# Create the training and validation directories.
Move 80% of the data to the training directory for each class.
"""

tf.keras.preprocessing.image.ImageDataGenerator()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## Creat data generator"""

output = '/content/output'
# Create "training" and "validation" directories
training_dir = os.path.join(output, "train")
validation_dir = os.path.join(output, "val")

data_gen_train = ImageDataGenerator(rescale=1/255.0)
data_gen_test = ImageDataGenerator(rescale=1/255.0)

train_generator = data_gen_train.flow_from_directory(training_dir,target_size=(128,128),batch_size=128,class_mode='binary')

test_generator = data_gen_test.flow_from_directory(validation_dir,target_size=(128,128),batch_size=128,class_mode='binary')

train_size = train_generator.n
val_size = test_generator.n
sample_count = train_size + val_size
print("The train_size is ",train_size)
print("The val_size is ", val_size)
print("The total sample_count is ", sample_count)

"""# Model building"""

# Load VGG19
from tensorflow.keras.applications import VGG19

conv_base = VGG19(weights='imagenet',
                  include_top=False,
                  input_shape=(150, 150, 3))

# Freeze VGG19 layers
# conv_base.trainable = False

conv_base.summary()

#reshaping the image
img_shape = (150,150,3)

conv_base.summary()

#Model frezing (to enable us apply them on our own custom task)
conv_base.trainable = False

"""## Extracting features from VGG19"""

#input layer  = [(None, 150, 150, 3)]
#target_size=(150, 150)
#last layer =  maxpooling  = (None, 4, 4, 512)
#shape=(sample_count, 4, 4, 512)

datagen = ImageDataGenerator(rescale=1./255)
batch_size = 16

def extract_features(directory, sample_count):
  '''Function to extract features from images, given the directory of the images and the sample count

  Parameters:
  -----------

  directory: str, this is the directory to the different classes of images
  sample_count: int, this is the total number of samples images

  Returns:
  --------
  features (numpy array) and the corresponding label (numpy array)

  '''
  features = np.zeros(shape=(sample_count, 4, 4, 512))  # Must be equal to the output of the convolutional base
  labels = np.zeros(shape=(sample_count))
  # Preprocess data
  generator = datagen.flow_from_directory(directory,
                                          target_size=(150, 150),
                                          batch_size = batch_size,
                                          class_mode='binary')
  # Pass data through convolutional base
  i = 0
  for inputs_batch, labels_batch in generator:
      features_batch = conv_base.predict(inputs_batch)
      features[i * batch_size: (i + 1) * batch_size] = features_batch
      labels[i * batch_size: (i + 1) * batch_size] = labels_batch
      i += 1
      if i * batch_size >= sample_count:
          break
  return features, labels

start = time.time()  # record start time
train_features, train_labels = extract_features(training_dir, train_size)  # Agree with our small dataset size
validation_features, validation_labels = extract_features(validation_dir, val_size)
end = time.time()  # record end time
print('The execution time is:', (end-start) * 10**3, 'ms')

# Reshape training data and validation
train_features = train_features.reshape(train_size, 4*4*512)
validation_features = validation_features.reshape(val_size, 4*4*512)

# save to csv file
np.savetxt('X_train_dendrite.csv', train_features, delimiter=',')
np.savetxt('X_val_dendrite.csv', validation_features, delimiter=',')
np.savetxt('y_train_dendrite.csv', train_labels, delimiter=',')
np.savetxt('y_val_dendrite.csv', validation_labels, delimiter=',')

print(len(train_features[0]))

# # from numpy import loadtxt
train_features = np.loadtxt('X_train_dendrite.csv', delimiter=",")
train_labels = np.loadtxt('y_train_dendrite.csv', delimiter=",")
validation_features = np.loadtxt('X_val_dendrite.csv', delimiter=",")
validation_labels = np.loadtxt('y_val_dendrite.csv', delimiter=",")

"""# Model Training"""

# Define Evaluation Metric Functions
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score
from sklearn.metrics import precision_score, f1_score, ConfusionMatrixDisplay
from sklearn.metrics import classification_report


def evaluate_model(y_val, y_pred):
    """Function to evaluate model and return the metric of the model

    It returns a dictionary with the classification metrics.
    """
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred, average='weighted')
    recall = recall_score(y_val, y_pred, average='weighted')
    f1 = f1_score(y_val, y_pred, average='weighted')
    result = {"accuracy_score": accuracy,
              "precision_score": precision,
              "recall_score": recall,
              "f1_score": f1}
    return result


def plot_confusion_matrix(y_val, y_pred, label):
    '''function to plot confusion matrix

    Args
    y_val: array. The validation set of the target variable.
    y_pred: array. Model's prediction.
    label: list. A list containing all the classes in the target variable

    Returns
    It returns a plot of the confusion matrix
    '''
    cm = confusion_matrix(y_val, y_pred)
    fig, ax = plt.subplots(figsize=(5,5))
    ConfusionMatrixDisplay(cm, display_labels=label).plot(ax=ax, values_format='', xticks_rotation='vertical')
    plt.show()


def display_predictions(y_test, y_pred):
    """
    Display actual values and model predictions in a Pandas DataFrame for the first 10 instances.

    Args:
    y_test: true labels of the test set
    y_pred: model's prediction

    Returns:
    Pandas DataFrame containing actual values and model predictions for the first 10 instances
    """
    df_results = pd.DataFrame({"Actual": y_test[:10],
                               "Prediction": y_pred[:10]})
    return df_results


# class labels
label = ['Non-Dendritic', 'Dendritic']
RANDOM_STATE = 1

"""# Model Comparison"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier as KNeighborClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier as KNeighborClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV

models = [LogisticRegression(max_iter=1000), SVC(kernel='linear'), KNeighborClassifier(), RandomForestClassifier(random_state=42)]

# Define separate parameter grids for each model
param_grids = {
    LogisticRegression: {'C': [0.01, 0.1, 1, 10], 'penalty': ['l2']},  # Parameters for LogisticRegression
    SVC: {'C': [0.01, 0.1], 'kernel': ['linear']},  # Parameters for SVC
    KNeighborClassifier: {'n_neighbors': [3, 5, 7, 9]},  # Parameters for KNeighborClassifier
    RandomForestClassifier: {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}  # Parameters for RandomForestClassifier
}

def compare_models_train_test():
  for model in models:

    X_train, X_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)
    #best_model.fit(X_train, y_train)

    # Get the appropriate parameter grid for the current model
    parameters = param_grids.get(type(model), {})

    # Fine-tune model
    grid_search = GridSearchCV(estimator=model,
                               param_grid=parameters, # Use the correct parameter grid
                               scoring='f1',
                               cv=5,
                               n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # Train the model
    best_model = grid_search.best_estimator_

    # Evaluate the model on the training set
    print("Training Result - {}".format(model))
    #print(evaluate_model(y_train, best_model.predict(X_train))) # Assuming evaluate_model is defined elsewhere


    # Evaluate the model on training data
    train_predictions = best_model.predict(X_train)
    train_accuracy = accuracy_score(y_train, train_predictions)
    train_accuracy = train_accuracy*100
    train_accuracy = round(train_accuracy, 2)
    print(f"Training Accuracy for {model}: {train_accuracy}%")

    # Evaluate the model on validation data
    val_predictions = best_model.predict(X_test)
    val_accuracy = accuracy_score(y_test, val_predictions)
    val_accuracy = val_accuracy*100
    val_accuracy = round(val_accuracy, 2)
    print(f"Validation Accuracy for {model}: {val_accuracy}%")

    # Confusion matrix
    plot_confusion_matrix(y_test, val_predictions, label)
    print(classification_report(y_test, val_predictions))

compare_models_train_test()