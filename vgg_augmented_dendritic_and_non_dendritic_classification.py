# -*- coding: utf-8 -*-
"""VGG - Augmented Dendritic and non_Dendritic Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JkdrQfRDwUW_tVK6C_A9sIK3-gG_xyQS
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import skimage.io
import os
import keras
import cv2
import os
import glob
# %matplotlib inline
import shutil
from shutil import copyfile
import random
import time
import cv2

# tensorflow Libraries
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
# from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# sklearn Libraries
import sklearn
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# Metrics Libraries
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score
from sklearn.metrics import precision_score, f1_score, ConfusionMatrixDisplay
from sklearn.metrics import classification_report

"""# Import dataset from google drive"""

# Download dataset
!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/nyb6mycvfd-1.zip

#unzip the dataset
!unzip -q /content/nyb6mycvfd-1.zip

"""# Image augmentation"""

from tensorflow.keras.utils import array_to_img, img_to_array, load_img
#from tensorflow.keras.utils import ImageDataGenerator, array_to_img, img_to_array, load_img
datagen = ImageDataGenerator(rotation_range =15,
                         width_shift_range = 0.2,
                         height_shift_range = 0.2,
                         rescale=1./255,
                         shear_range=0.2,
                         zoom_range=0.2,
                         horizontal_flip = True,
                         fill_mode = 'nearest',
                         data_format='channels_last',
                         brightness_range=[0.5, 1.5])

"""# Create Training and Test data directory"""

try:
  os.mkdir('Train_Dendrite')
except:
  print("Folder already exists")

try:
  os.mkdir('Test_Dendrite')
except:
  print("Folder already exists")

try:
  os.mkdir('Train_Non_Dendrite')
except:
  print("Folder already exists")

try:
  os.mkdir('Test_Non_Dendrite')
except:
  print("Folder already exists")

import os
import shutil
import random

# Define paths for the directories
dendritic_dir = "/content/Dendritic"
non_dendritic_dir = "/content/Non-Dendritic"
train_dendrite_dir = "/content/Train_Dendrite"
test_dendrite_dir = "/content/Test_Dendrite"
train_non_dendrite_dir = "/content/Train_Non_Dendrite"
test_non_dendrite_dir = "/content/Test_Non_Dendrite"

# Function to split and move files
def split_and_move_files(src_dir, train_dir, test_dir, train_ratio=0.8, random_state=42):
    # Get a list of all files in the source directory
    files = os.listdir(src_dir)
    random.seed(random_state) #added seed for reproducibility
    random.shuffle(files)  # Shuffle for randomness

    # Calculate split index
    split_index = int(len(files) * train_ratio)

    # Split into training and testing sets
    train_files = files[:split_index]
    test_files = files[split_index:]

    # Create the train and test directories if they don't exist
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # Move files to train directory
    for file_name in train_files:
        source_path = os.path.join(src_dir, file_name)
        destination_path = os.path.join(train_dir, file_name)
        if os.path.exists(source_path): #check if the file exists
            shutil.move(source_path, destination_path)

    # Move files to test directory
    for file_name in test_files:
        source_path = os.path.join(src_dir, file_name)
        destination_path = os.path.join(test_dir, file_name)
        if os.path.exists(source_path): #check if the file exists
            shutil.move(source_path, destination_path)

# Move files from Dendritic folder
split_and_move_files(dendritic_dir, train_dendrite_dir, test_dendrite_dir)

# Move files from Non-Dendritic folder
split_and_move_files(non_dendritic_dir, train_non_dendrite_dir, test_non_dendrite_dir)

"""# Data Aumentation"""

# prompt: generate code to create two folders and name it Augment_Dendrite and Augment_Non_Dendrite

try:
  os.mkdir('Augment_Dendrite')
except:
  print("Folder already exists")

try:
  os.mkdir('Augment_Non_Dendrite')
except:
  print("Folder already exists")

#img_dir_Den = "/content/Train_Dendrite" # Enter Directory of all images
img_dir_Den = train_dendrite_dir
data_path = os.path.join(img_dir_Den,'*g')
files = glob.glob(data_path)
data = []
for f1 in files:
    img = cv2.imread(f1)
    data.append(img)

    x = img_to_array(img)
    x = x.reshape((1,) + x.shape)

    i = 0
    #path, dirs, files = next(os.walk("/content/Train_Dendrite"))
    path, dirs, files = next(os.walk(train_dendrite_dir))
    file_count = len(files) #to find number of files in folder

    for batch in datagen.flow (x, batch_size=1, save_to_dir =r'/content/Augment_Dendrite',save_prefix="a",save_format='jpg'):
      i+=1
      if i>53:
        break

#img_dir_Non_Den = "/content/Train_Non_Dendrite" # Enter Directory of all images
img_dir_Non_Den = train_non_dendrite_dir
data_path_N = os.path.join(img_dir_Non_Den,'*g')
files = glob.glob(data_path_N)
data_N = []
for f1 in files:
    img = cv2.imread(f1)
    data_N.append(img)

    x = img_to_array(img)
    x = x.reshape((1,) + x.shape)

    i = 0
    #path, dirs, files = next(os.walk("/content/Train_Non_Dendrite"))
    path, dirs, files = next(os.walk(train_non_dendrite_dir))
    file_count = len(files) #to find number of files in folder

    for batch in datagen.flow (x, batch_size=1, save_to_dir =r'/content/Augment_Non_Dendrite',save_prefix="a",save_format='jpg'):
      i+=1
      if i>20:
        break



"""## Image preprocessing"""

# set path to dendritic micrograph images
tr_dendritic_dir = '/content/Augment_Dendrite'
# set path to non dendritic micrograph images
tr_non_dendritic_dir = '/content/Augment_Non_Dendrite'

train_dendrite_dir = "/content/Train_Dendrite"
test_dendrite_dir = "/content/Test_Dendrite"

#Print the total number of images in each directory
print("The total number of training dendritic images are", len(os.listdir(tr_dendritic_dir)))
print("The total number of training non dendritic images are", len(os.listdir(tr_non_dendritic_dir)))
print("The total number of testing dendritic images are", len(os.listdir(test_dendrite_dir)))
print("The total number of testing non dendritic images are", len(os.listdir(test_non_dendrite_dir)))

# merge dedritic and non_dendritic images
merged_list = test_dendrite_dir + tr_dendritic_dir + test_non_dendrite_dir + tr_non_dendritic_dir
print("The total number of images in the merged list are", len(merged_list))

# Plotting the images in the merged list
for i, img_path in enumerate(merged_list):
    # getting the filename from the directory
    data = img_path.split('/', 7)[3]
    # creating a subplot of images with the no. of rows and colums with index no
    sp = plt.subplot(ROWS, COLS, i+1)
    # turn off axis
    sp.axis('Off')
    # reading the image data to an array
    img = mpimg.imread(img_path)
    # setting title of plot as the filename
    sp.set_title(data, fontsize=10)
    # displaying data as image
    plt.imshow(img, cmap='gray')

plt.show()  # display the plot

# Plot class distribution
plt.figure(figsize=(8,2))
x = np.arange(2)
y = [len(os.listdir(dendritic_dir)), len(os.listdir(non_dendritic_dir))]
plt.barh(x, y)
plt.yticks(x, ["Dendritic", "Non-Dendritic"], fontsize=10)
plt.text(y[0]+5, x[0], y[0], fontsize=8)
plt.text(y[1]+5, x[1], y[1], fontsize=8)
plt.title("Distribution of Classes in the Dataset", fontsize=14);

"""# Create the training and validation directories.
Move 80% of the data to the training directory for each class.
"""

try:
  os.mkdir('main_dir')
except:
  print("Folder already exists")

# Move Dendritic folder
try:
  shutil.move('/content/Augment_Dendrite', '/content/main_dir')
except:
  print("Folder already exists")

# Move Non-Dendritic folder
try:
  shutil.move('/content/Augment_Non_Dendrite', '/content/main_dir')
except:
  print("Folder already exists")

main_dir = '/content/main_dir'

! pip install split-folders

import splitfolders
import os
print(os.listdir('/content/main_dir'))
splitfolders.ratio('/content/main_dir', output="output", seed=1337, ratio=(.8, .2), group_prefix=None)

output = '/content/output'
# Create "training" and "validation" directories
training_dir = os.path.join(output, "train")
validation_dir = os.path.join(output, "val")

tf.keras.preprocessing.image.ImageDataGenerator()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## Creat data generator"""

data_gen_train = ImageDataGenerator(rescale=1/255.0)
data_gen_test = ImageDataGenerator(rescale=1/255.0)

train_generator = data_gen_train.flow_from_directory(training_dir,target_size=(128,128),batch_size=128,class_mode='binary')

test_generator = data_gen_test.flow_from_directory(validation_dir,target_size=(128,128),batch_size=128,class_mode='binary')

train_size = train_generator.n
val_size = test_generator.n
sample_count = train_size + val_size
print("The train_size is ",train_size)
print("The val_size is ", val_size)
print("The total sample_count is ", sample_count)

"""# Model building"""

# Load VGG19
from tensorflow.keras.applications import VGG19

conv_base = VGG19(weights='imagenet',
                  include_top=False,
                  input_shape=(150, 150, 3))

# Freeze VGG19 layers
# conv_base.trainable = False

conv_base.summary()

#reshaping the image
img_shape = (150,150,3)

#Model frezing (to enable us apply them on our own custom task)
conv_base.trainable = False

"""## Defining the transfer learning model

## Extracting features from VGG19
"""

#input layer  = [(None, 150, 150, 3)]
#target_size=(150, 150)
#last layer =  maxpooling  = (None, 4, 4, 512)
#shape=(sample_count, 4, 4, 512)

datagen = ImageDataGenerator(rescale=1./255)
batch_size = 16

def extract_features(directory, sample_count):
  '''Function to extract features from images, given the directory of the images and the sample count

  Parameters:
  -----------

  directory: str, this is the directory to the different classes of images
  sample_count: int, this is the total number of samples images

  Returns:
  --------
  features (numpy array) and the corresponding label (numpy array)

  '''
  features = np.zeros(shape=(sample_count, 4, 4, 512))  # Must be equal to the output of the convolutional base
  labels = np.zeros(shape=(sample_count))
  # Preprocess data
  generator = datagen.flow_from_directory(directory,
                                          target_size=(150, 150),
                                          batch_size = batch_size,
                                          class_mode='binary')
  # Pass data through convolutional base
  i = 0
  for inputs_batch, labels_batch in generator:
      features_batch = conv_base.predict(inputs_batch)
      features[i * batch_size: (i + 1) * batch_size] = features_batch
      labels[i * batch_size: (i + 1) * batch_size] = labels_batch
      i += 1
      if i * batch_size >= sample_count:
          break
  return features, labels

start = time.time()  # record start time
train_features, train_labels = extract_features(training_dir, train_size)  # Agree with our small dataset size
validation_features, validation_labels = extract_features(validation_dir, val_size)
end = time.time()  # record end time
print('The execution time is:', (end-start) * 10**3, 'ms')

# Reshape training data and validation
train_features = train_features.reshape(train_size, 4*4*512)
validation_features = validation_features.reshape(val_size, 4*4*512)

# save to csv file
np.savetxt('X_train_dendrite.csv', train_features, delimiter=',')
np.savetxt('X_val_dendrite.csv', validation_features, delimiter=',')
np.savetxt('y_train_dendrite.csv', train_labels, delimiter=',')
np.savetxt('y_val_dendrite.csv', validation_labels, delimiter=',')

# clear backend session of tf
# tf.keras.backend.clear_session()

print(len(train_features[0]))

print(train_labels)

# # from numpy import loadtxt
train_features = np.loadtxt('X_train_dendrite.csv', delimiter=",")
train_labels = np.loadtxt('y_train_dendrite.csv', delimiter=",")
validation_features = np.loadtxt('X_val_dendrite.csv', delimiter=",")
validation_labels = np.loadtxt('y_val_dendrite.csv', delimiter=",")

"""# Fine Tuning"""

# Define Evaluation Metric Functions
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score
from sklearn.metrics import precision_score, f1_score, ConfusionMatrixDisplay
from sklearn.metrics import classification_report


def evaluate_model(y_val, y_pred):
    """Function to evaluate model and return the metric of the model

    It returns a dictionary with the classification metrics.
    """
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred, average='weighted')
    recall = recall_score(y_val, y_pred, average='weighted')
    f1 = f1_score(y_val, y_pred, average='weighted')
    loss= binary_crossentropy(y_val, y_pred)
    result = {"accuracy_score": accuracy,
              "precision_score": precision,
              "recall_score": recall,
              "f1_score": f1,
              "loss_score": loss}
    return result


def plot_confusion_matrix(y_val, y_pred, label):
    '''function to plot confusion matrix

    Args
    y_val: array. The validation set of the target variable.
    y_pred: array. Model's prediction.
    label: list. A list containing all the classes in the target variable

    Returns
    It returns a plot of the confusion matrix
    '''
    cm = confusion_matrix(y_val, y_pred)
    fig, ax = plt.subplots(figsize=(5,5))
    ConfusionMatrixDisplay(cm, display_labels=label).plot(ax=ax, values_format='', xticks_rotation='vertical')
    plt.show()

def plot_loss(y_val, y_pred, label):
    '''function to plot confusion matrix

    Args
    y_val: array. The validation set of the target variable.
    y_pred: array. Model's prediction.
    label: list. A list containing all the classes in the target variable

    Returns
    It returns a plot of the confusion matrix
    '''
    loss = plot_loss(y_val, y_pred)
    fig, ax = plt.subplots(figsize=(5,5))
    ConfusionMatrixDisplay(loss_score, display_labels=label).plot(ax=ax, values_format='', xticks_rotation='vertical')
    plt.show()


def display_predictions(y_test, y_pred):
    """
    Display actual values and model predictions in a Pandas DataFrame for the first 10 instances.

    Args:
    y_test: true labels of the test set
    y_pred: model's prediction

    Returns:
    Pandas DataFrame containing actual values and model predictions for the first 10 instances
    """
    df_results = pd.DataFrame({"Actual": y_test[:10],
                               "Prediction": y_pred[:10]})
    return df_results


# class labels
label = ['Non-Dendritic', 'Dendritic']
RANDOM_STATE = 1

"""# Model Comparison"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier as KNeighborClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier as KNeighborClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV

models = [LogisticRegression(max_iter=1000), SVC(kernel='linear'), KNeighborClassifier(), RandomForestClassifier(random_state=42)]

# Define separate parameter grids for each model
param_grids = {
    LogisticRegression: {'C': [0.01, 0.1, 1, 10], 'penalty': ['l2']},  # Parameters for LogisticRegression
    SVC: {'C': [0.01, 0.1], 'kernel': ['linear']},  # Parameters for SVC
    KNeighborClassifier: {'n_neighbors': [3, 5, 7, 9]},  # Parameters for KNeighborClassifier
    RandomForestClassifier: {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}  # Parameters for RandomForestClassifier
}

def compare_models_train_test():
  for model in models:

    X_train, X_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)
    #best_model.fit(X_train, y_train)

    # Get the appropriate parameter grid for the current model
    parameters = param_grids.get(type(model), {})

    # Fine-tune model
    grid_search = GridSearchCV(estimator=model,
                               param_grid=parameters, # Use the correct parameter grid
                               scoring='f1',
                               cv=5,
                               n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # Train the model
    best_model = grid_search.best_estimator_

    # Evaluate the model on the training set
    print("Training Result - {}".format(model))
    #print(evaluate_model(y_train, best_model.predict(X_train))) # Assuming evaluate_model is defined elsewhere


    # Evaluate the model on training data
    train_predictions = best_model.predict(X_train)
    train_accuracy = accuracy_score(y_train, train_predictions)
    train_accuracy = train_accuracy*100
    train_accuracy = round(train_accuracy, 2)
    print(f"Training Accuracy for {model}: {train_accuracy}%")

    # Evaluate the model on validation data
    val_predictions = best_model.predict(X_test)
    val_accuracy = accuracy_score(y_test, val_predictions)
    val_accuracy = val_accuracy*100
    val_accuracy = round(val_accuracy, 2)
    print(f"Validation Accuracy for {model}: {val_accuracy}%")

    # Confusion matrix
    plot_confusion_matrix(y_test, val_predictions, label)
    print(classification_report(y_test, val_predictions))

compare_models_train_test()